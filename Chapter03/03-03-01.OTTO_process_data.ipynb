{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading otto-recommender-system, 2080714974 bytes compressed\n",
      "[==                                                ] 122880000 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======                                           ] 319037440 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============                                      ] 514990080 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================                                 ] 710901760 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================                             ] 906690560 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================                        ] 1102602240 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================                   ] 1299578880 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================================               ] 1495490560 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================          ] 1690501120 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================     ] 1886208000 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 2080714974 bytes downloaded\n",
      "Downloaded and uncompressed: otto-recommender-system\n",
      "Downloading otto-full-optimized-memory-footprint, 1168318093 bytes compressed\n",
      "[=====                                             ] 122839040 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============                                     ] 321945600 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================                            ] 518062080 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================                    ] 712826880 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================            ] 907960320 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================================    ] 1098096640 bytes downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 1168318093 bytes downloaded\n",
      "Downloaded and uncompressed: otto-full-optimized-memory-footprint\n",
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
    "# TO THE CORRECT LOCATION (kaggle/input) IN YOUR NOTEBOOK,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tempfile import NamedTemporaryFile\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import unquote, urlparse\n",
    "from urllib.error import HTTPError\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "CHUNK_SIZE = 40960\n",
    "DATA_SOURCE_MAPPING = 'otto-recommender-system:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F38760%2F4493939%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240914%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240914T080551Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D018c258861bbd7763382c0dd7a3218290d2b3de77259c2e1015a19d87d4d22f710ffbd1010015111b89a5cfa25e6ffdd35ed6b02b4e3d0bff0e6149e69b3d04566a44549ae361a285f2c03930ecd10c71e18e9b256d0ae8bda350208da18f3c672fa7baaeef36c6931d55ebdfbe44722e20af250058b0fa651eeed2067d3589e8255a51d3e755080ae6d70f14cdb7adb0b305e928fea1951ab15584af355ebecbcda956895550a8901ea5cfa2dd5e0e069c18e0e7006d9bc31dabf85f863a216c704ca60aa9ef19db8fb15a3e5cfe27b59a1dde012441ab22f80e505e1c73ea3d1c5ef25968f6b99d8175c7b50093356539c8072c578ecd54762800851f4ba93,otto-full-optimized-memory-footprint:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2601572%2F4474043%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240914%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240914T080551Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D45fda05a433f1b541e4fb41316790820b476378ed7c047f3fc7b196e262f1911f13412dca8dd995fc571a3d958268e18d88f2e5db6ae5cd04fb9a0bb9fb11949036ad20fdd4f23044d44e28ce23dfffa58261924b421017a52342e517d96846d62c4ab0c348c6856d54ca3ce794605fe4dac64a865bd2ba6351e15a501088f31a85b4819466abfbb4861b109529d49ef452368f94d94a8953c6003a336cfdc103bc66ec7faea968aba08dbb3539bca95cb5336cf8cf17316eeca6e4bdb2b1ace613f63ca6f319966c89009866d959a4ae4782c70dceb3df37e797d5e13318d0623dffb4aea5cb7c7f8d31cbc01fcc971d008f736aa3c0c579b9dae3d0c0f28ea'\n",
    "\n",
    "KAGGLE_INPUT_PATH='kaggle/input'\n",
    "KAGGLE_WORKING_PATH='kaggle/working'\n",
    "KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "!umount kaggle/input/ 2> /dev/null\n",
    "shutil.rmtree('kaggle/input', ignore_errors=True)\n",
    "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "try:\n",
    "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "try:\n",
    "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "    directory, download_url_encoded = data_source_mapping.split(':')\n",
    "    download_url = unquote(download_url_encoded)\n",
    "    filename = urlparse(download_url).path\n",
    "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "    try:\n",
    "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "            total_length = fileres.headers['content-length']\n",
    "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "            dl = 0\n",
    "            data = fileres.read(CHUNK_SIZE)\n",
    "            while len(data) > 0:\n",
    "                dl += len(data)\n",
    "                tfile.write(data)\n",
    "                done = int(50 * dl / int(total_length))\n",
    "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "                sys.stdout.flush()\n",
    "                data = fileres.read(CHUNK_SIZE)\n",
    "            if filename.endswith('.zip'):\n",
    "              with ZipFile(tfile) as zfile:\n",
    "                zfile.extractall(destination_path)\n",
    "            else:\n",
    "              with tarfile.open(tfile.name) as tarfile:\n",
    "                tarfile.extractall(destination_path)\n",
    "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "    except HTTPError as e:\n",
    "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "        continue\n",
    "    except OSError as e:\n",
    "        print(f'Failed to load {download_url} to path {destination_path}')\n",
    "        continue\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"session\":12899779,\"events\":[{\"aid\":59625,\"ts\":1661724000278,\"type\":\"clicks\"}]}\n",
      "{\"session\":12899780,\"events\":[{\"aid\":1142000,\"ts\":1661724000378,\"type\":\"clicks\"}\n",
      "{\"session\":12899781,\"events\":[{\"aid\":141736,\"ts\":1661724000559,\"type\":\"clicks\"},\n",
      "{\"session\":12899782,\"events\":[{\"aid\":1669402,\"ts\":1661724000568,\"type\":\"clicks\"}\n",
      "{\"session\":12899783,\"events\":[{\"aid\":255297,\"ts\":1661724000572,\"type\":\"clicks\"},\n",
      "{\"session\":12899784,\"events\":[{\"aid\":1036375,\"ts\":1661724000604,\"type\":\"clicks\"}\n",
      "{\"session\":12899785,\"events\":[{\"aid\":1784451,\"ts\":1661724000809,\"type\":\"clicks\"}\n",
      "{\"session\":12899786,\"events\":[{\"aid\":955252,\"ts\":1661724001174,\"type\":\"clicks\"},\n",
      "{\"session\":12899787,\"events\":[{\"aid\":1682750,\"ts\":1661724001617,\"type\":\"clicks\"}\n",
      "{\"session\":12899788,\"events\":[{\"aid\":245131,\"ts\":1661724001619,\"type\":\"clicks\"},\n"
     ]
    }
   ],
   "source": [
    "!head kaggle/input/otto-recommender-system/test.jsonl | cut -c -80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.perf_counter()\n",
    "        self.interval = self.end - self.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'json       pd: 16.18298s cudf:  1.43771s'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"kaggle/input/otto-recommender-system/test.jsonl\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu = pd.read_json(file_path, lines=True,  orient=\"records\")\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu = cudf.read_json(file_path, lines=True)\n",
    "\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"json\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parquet    pd:  4.80043s cudf:  0.28367s'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"data/test.parquet\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu.to_parquet(file_path, index=False)\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu.to_parquet(file_path, index=False)\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"parquet\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "id2type = ['clicks', 'carts', 'orders'] # I have analyzed the data\n",
    "                                          # and so I know we can expect these event types\n",
    "type2id = {a: i for i, a in enumerate(id2type)}\n",
    "\n",
    "id2type, type2id\n",
    "\n",
    "pd.to_pickle(id2type, 'id2type.pkl')\n",
    "pd.to_pickle(type2id, 'type2id.pkl')\n",
    "\n",
    "def jsonl_to_df(fn):\n",
    "    sessions = []\n",
    "    aids = []\n",
    "    tss = []\n",
    "    types = []\n",
    "\n",
    "    chunks = pd.read_json(fn, lines=True, chunksize=500_000)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        for row_idx, session_data in chunk.iterrows():\n",
    "            num_events = len(session_data.events)\n",
    "            sessions += ([session_data.session] * num_events)\n",
    "            for event in session_data.events:\n",
    "                aids.append(event['aid'])\n",
    "                tss.append(event['ts'])\n",
    "                types.append(type2id[event['type']])\n",
    "        \n",
    "    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})\n",
    "\n",
    "with Timer() as t_pandas:\n",
    "    train_df = jsonl_to_df('kaggle/input/otto-recommender-system/train.jsonl')\n",
    "\"{:10s} pandas: {:>8.5f}s\".format(\"parquet\", t_pandas.interval)\n",
    "\n",
    "train_df.type = train_df.type.astype(np.uint8) # a tiny bit of further memory footprint optimization\n",
    "train_df.to_parquet('data/train.parquet', index=False)\n",
    "#train_df.to_csv('train.csv', index=False)\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as cp  # CuPy를 사용하기 위해 import\n",
    "import os\n",
    "\n",
    "def jsonl_to_df(fn, output_dir='output_chunks', chunk_size=100_000, split_size=1_000_000):\n",
    "    sessions = []\n",
    "    aids = []\n",
    "    tss = []\n",
    "    types = []\n",
    "    \n",
    "    # 각 파일이 저장될 chunk 번호를 저장하는 변수\n",
    "    chunk_num = 0\n",
    "\n",
    "    # 저장할 디렉토리가 없으면 생성\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    chunks = pd.read_json(fn, lines=True, chunksize=chunk_size)\n",
    "\n",
    "    # row count tracking\n",
    "    total_rows = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for row_idx, session_data in chunk.iterrows():\n",
    "            num_events = len(session_data.events)\n",
    "            # NumPy 대신 CuPy를 사용하여 배열을 처리\n",
    "            sessions += cp.asnumpy(cp.repeat(cp.array(session_data.session), num_events)).tolist()\n",
    "            for event in session_data.events:\n",
    "                aids.append(event['aid'])\n",
    "                tss.append(event['ts'])\n",
    "                types.append(type2id[event['type']])\n",
    "                \n",
    "            total_rows += num_events\n",
    "\n",
    "            # 일정 크기만큼 데이터가 모이면 Parquet로 저장\n",
    "            if total_rows >= split_size:\n",
    "                # CuPy 배열을 사용하여 DataFrame 생성\n",
    "                df = pd.DataFrame(data={\n",
    "                    'session': sessions,\n",
    "                    'aid': aids,\n",
    "                    'ts': tss,\n",
    "                    'type': cp.asnumpy(cp.array(types)).tolist()  # CuPy 배열을 다시 NumPy로 변환하여 pandas에서 처리\n",
    "                })\n",
    "                \n",
    "                # 메모리 절약을 위해 데이터 타입을 변환\n",
    "                df.type = df.type.astype(cp.uint8)  # CuPy의 uint8을 사용\n",
    "                \n",
    "                # Parquet 파일로 저장\n",
    "                parquet_file = os.path.join(output_dir, f'chunk_{chunk_num}.parquet')\n",
    "                df.to_parquet(parquet_file, index=False)\n",
    "                \n",
    "                # 다음 chunk 파일을 위해 초기화\n",
    "                sessions = []\n",
    "                aids = []\n",
    "                tss = []\n",
    "                types = []\n",
    "                total_rows = 0\n",
    "                chunk_num += 1\n",
    "    \n",
    "    # 마지막으로 남은 데이터 저장\n",
    "    if sessions:\n",
    "        df = pd.DataFrame(data={\n",
    "            'session': sessions,\n",
    "            'aid': aids,\n",
    "            'ts': tss,\n",
    "            'type': cp.asnumpy(cp.array(types)).tolist()\n",
    "        })\n",
    "        df.type = df.type.astype(cp.uint8)\n",
    "        parquet_file = os.path.join(output_dir, f'chunk_{chunk_num}.parquet')\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "\n",
    "    print(f\"Data saved in {output_dir} in {chunk_num + 1} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in train_chunks in 44 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parquet    numpy: 2457.11536s cupy: 4688.99023s'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Timer() as t_cupy:\n",
    "    jsonl_to_df('kaggle/input/otto-recommender-system/train.jsonl', output_dir='train_chunks', chunk_size=500_000, split_size=5_000_000)\n",
    "\"{:10s} numpy: {:>8.5f}s cupy: {:>8.5f}s\".format(\"parquet\", t_pandas.interval, t_cupy.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "total_df = cudf.read_parquet('train_chunks/*.parquet')\n",
    "total_df.to_parquet('data/train2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.6G\n",
      "4.0K drwxr-xr-x 2 root root 4.0K Sep 15 06:55 .\n",
      "4.0K drwxrwxrwx 1 root root 4.0K Sep 15 06:55 ..\n",
      "1.1G -rw-r--r-- 1 root root 1.1G Aug  2  2022 numbers.csv\n",
      "522M -rw-r--r-- 1 root root 522M Aug  2  2022 numbers.parquet\n",
      "3.3M -rw-r--r-- 1 root root 3.3M Aug  2  2022 sample_data2.csv\n",
      "3.3M -rw-r--r-- 1 root root 3.3M Aug  2  2022 sample_data3.csv\n",
      " 12M -rw-r--r-- 1 root root  12M Aug  2  2022 sample_json.txt\n",
      " 31M -rw-r--r-- 1 root root  31M Sep 15 04:55 test.parquet\n",
      "2.0G -rw-r--r-- 1 root root 2.0G Sep 15 06:55 train2.parquet\n",
      "2.1G -rw-r--r-- 1 root root 2.1G Sep 15 05:37 train.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -al data -sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.1G\n",
      "47M chunk_0.parquet   49M chunk_23.parquet  49M chunk_37.parquet\n",
      "48M chunk_10.parquet  49M chunk_24.parquet  49M chunk_38.parquet\n",
      "48M chunk_11.parquet  49M chunk_25.parquet  49M chunk_39.parquet\n",
      "48M chunk_12.parquet  49M chunk_26.parquet  47M chunk_3.parquet\n",
      "48M chunk_13.parquet  49M chunk_27.parquet  49M chunk_40.parquet\n",
      "48M chunk_14.parquet  49M chunk_28.parquet  49M chunk_41.parquet\n",
      "48M chunk_15.parquet  49M chunk_29.parquet  49M chunk_42.parquet\n",
      "49M chunk_16.parquet  47M chunk_2.parquet   18M chunk_43.parquet\n",
      "49M chunk_17.parquet  49M chunk_30.parquet  47M chunk_4.parquet\n",
      "48M chunk_18.parquet  49M chunk_31.parquet  47M chunk_5.parquet\n",
      "49M chunk_19.parquet  49M chunk_32.parquet  47M chunk_6.parquet\n",
      "47M chunk_1.parquet   49M chunk_33.parquet  48M chunk_7.parquet\n",
      "49M chunk_20.parquet  49M chunk_34.parquet  48M chunk_8.parquet\n",
      "49M chunk_21.parquet  49M chunk_35.parquet  48M chunk_9.parquet\n",
      "49M chunk_22.parquet  49M chunk_36.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls train_chunks -sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
